# Apache Spark Actions & DataFrame Operations ‚Äì Clean Notes

---

## ‚ñ∂Ô∏è Spark Actions

‚Üí Actions **instruct Spark to compute a result**  
‚Üí To trigger computation, **an action must be executed**

Example:
```python
df.count()   # action
````

Notes:

* Transformations define *what* to do
* **Actions trigger execution**

---

### Types of Actions

```
Actions
 /      |        \
View   Collect   Write
```

‚Üí **Action to View**

* Displays data on the console
* Example: `show()`

‚Üí **Action to Collect**

* Collects data to native objects in the respective language
* Example: collecting Spark output into a Python object

‚Üí **Action to Write**

* Writes output data to external systems
* Examples:

  * SQL tables
  * Files
  * Excel, CSV, Parquet, etc.

---

## üí§ Lazy Evaluation

‚Üí Spark sequences transformations to **minimize computation**
‚Üí Spark does **not** execute transformations immediately
‚Üí Execution happens **only when an action is called**

This behavior is called **lazy evaluation**

Benefit:

* End-to-end **optimization of the entire data flow**

---

## üîÑ DataFrame Transformations ‚Äì Rows

### Filter Rows

Two equivalent approaches:

```python
df.where("count < 2").show(2)
```

```python
df.filter(col("count") < 2).show(2)
```

---

### Unique Rows

```python
df.select("column_name1", "column_name2").distinct().count()
```

Notes:

* If two columns are selected, `distinct()` returns **unique combinations**

---

### Sorting Rows

‚Üí Default order is **ascending**

Two methods:

```python
df.sort("count").show(5)
```

```python
df.orderBy("count").show(5)
```

Descending order:

```python
from pyspark.sql.functions import desc, asc
df.orderBy(desc("count")).show(5)
```

---

### Random Sampling

‚Üí Used to pick **random records**

Parameters:

* `withReplacement` ‚Üí `True` / `False`
* `fraction` ‚Üí proportion of data
* `seed` ‚Üí for reproducibility

Example:

```python
df.sample(withReplacement=False, fraction=0.1, seed=42)
```

---

### Random Split

```python
df1, df2 = df.randomSplit([0.25, 0.75], seed=42)
```

‚Üí Splits data into multiple DataFrames
‚Üí Fractions should sum approximately to **1**

---

### Multiple Conditions

‚Üí Multiple `where` conditions are treated as **AND** conditions

```python
df.where((col("a") > 1) & (col("b") < 5))
```

---

## üßæ Different Types of Data

* Boolean
* Numeric
* String
* Date & Timestamp
* Complex
* Null

---

### Boolean Operations

```
Boolean
 /    |    \
AND   OR   TRUE/FALSE
```

---

## üî¢ Numeric Operations

Supported operations:

* Add
* Subtract
* Mean
* Standard Deviation
* Correlation
* Rounding

### Rounding Functions

* `round()`
* `ceil()`
* `floor()`

Examples:

```python
round(2.4)  # 2
round(2.5)  # 3
floor(2.5)  # 2
ceil(2.4)   # 3
```

---

## üìä describe()

‚Üí Used to calculate **statistical summary**

Returns:

* count
* mean
* standard deviation
* min
* max

Example:

```python
df.describe().show()
```

---

## üîó Correlation

‚Üí Measures how one variable **influences another**
‚Üí Variables are DataFrame **columns**

---

## üî§ String Functions

Common string operations:

* Capitalize words
* Add / remove spaces
* Regular expressions

### Common Functions

* `initcap()` ‚Üí Capitalizes first letter of each word
* `lower()`
* `upper()`
* `lpad()` ‚Üí Left pad (based on total length)
* `rpad()` ‚Üí Right pad
* `trim()`
* `ltrim()`
* `rtrim()`

---

## üîç Regular Expressions

‚Üí Used for advanced pattern matching & replacements
‚Üí Works with string columns

---

## üìÖ Date & Timestamp

‚Üí Dates & timestamps are stored as **strings** internally
‚Üí Converted during execution

Example:

```python
date_df = spark.range(10) \
  .withColumn("today", current_date()) \
  .withColumn("now", current_timestamp())
```

---

## ‚úÖ Summary

* Actions trigger Spark execution
* Spark uses lazy evaluation for optimization
* DataFrame transformations are immutable
* Sampling, sorting, filtering enable efficient analysis
* Built-in functions simplify numeric, string, and date operations
