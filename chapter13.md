# Apache Spark â€“ Null Handling, Complex Types & Data Sources â€“ Clean Notes

---

## ğŸ§© Date & Utility Functions

â†’ Functions that are **not registered** cannot be used directly  
â†’ Return type should always **match the column type**

Common functions:
- `datediff()` â†’ Difference between two dates
- `to_date()` â†’ Convert string to date

---

## ğŸš« Null Handling in Spark

â†’ Presence of **null / empty values** makes optimization difficult in Spark  
â†’ Spark uses `na` package to handle missing values

### Two Ways to Handle Nulls

1. Explicitly **drop** nulls  
2. **Fill** null values

---

### Common Null Functions

â†’ **coalesce**
- Picks the **first non-null value** from a set of columns

â†’ **drop**
- Removes rows that contain null values

â†’ **fill**
- Fills one or more columns with specified values

â†’ **replace**
- Similar to regular expression replacement

---

## ğŸ§¬ Complex Data Types

### Struct
â†’ DataFrame inside a DataFrame  
â†’ Access fields using:
- Dot notation
- `getField()`

---

### Array
â†’ Collection of elements of the **same data type**

Useful functions:
- `split()`
- `array_length()` / `size()`
- `array_contains()`
- `explode()`

---

### Map
â†’ Keyâ€“value pairs  
â†’ Created using `create_map()`

---

## ğŸ‘¤ User Defined Functions (UDF)

â†’ UDFs can be written in **any supported language**  
â†’ Must be **registered** before use  
â†’ Can be reused across different Spark languages

---

## ğŸ“¥ Data Sources in Spark

Spark supports **all types of data**:

### Structured
- JDBC / ODBC connections

### Semi-Structured
- CSV
- JSON
- Parquet
- ORC

### Unstructured
- Plain text files

---

### Common External Sources

- Cassandra
- MongoDB
- HBase
- XML
- Redshift
- AWS (S3)

â†’ Spark provides built-in connectors to import data from these sources

---

## ğŸ“– Read API Structure

â†’ Spark uses **DataFrameReader** to read data

Key components:
- **Format**
- **Schema**
- **Read Mode**
- **Options** (format-specific)

---

### Read Modes

Spark supports **three read modes**:

1. **Permissive** (default)
   - Corrupt records are set to `null`
   - New column created for corrupt data

2. **DropMalformed**
   - Corrupt records are dropped

3. **FailFast**
   - Fails immediately
   - No DataFrame is created

---

## âœï¸ Write API Structure

â†’ Spark uses **DataFrameWriter** to write data

Components:
- Format (`text`, `csv`, `json`, etc.)
- Save Mode
- Options

---

### Save Modes

- **append** â†’ Appends data
- **overwrite** â†’ Overwrites existing data
- **errorIfExists** â†’ Throws error if data exists
- **ignore** â†’ Ignores write if data exists

---

## ğŸ“„ Semi-Structured Data: CSV Files

â†’ CSV = Comma Separated Values  
â†’ Each line represents one record  
â†’ Comma separates fields

---

### CSV Options

| Read / Write | Key |
|--------------|-----|
| Both | `sep` |
| Both | `header` |
| Read | `escape` |
| Read | `inferSchema` |
| Read | `ignoreLeadingWhiteSpace` |

---

## âš ï¸ Failures While Reading CSV Files

Common issues:
- Schema mismatch
- File name mismatch
- File does not exist

---

## ğŸ’¾ Writing CSV Files

Example:
```python
df.write \
  .format("csv") \
  .mode("overwrite") \
  .option("header", "true") \
  .save("path/to/output")
