
# Apache Spark Core Concepts â€“ Clean Notes

---

## âš¡ Apache Spark Overview

â†’ Spark is a tool for **parallel data processing** on computer clusters  
â†’ It is a **unified computing engine**  
â†’ Not limited to processing only structured data  

â†’ Spark is widely used for:
- Machine Learning
- Streaming analytics
- ETL batch processing

---

### Memory Handling in Spark

â†’ Spark tries to hold as much data as possible **in memory**  
â†’ If data does not fit in memory:
- Intermediate output is written to disk

---

## ğŸ“š Spark Libraries

â†’ Spark provides multiple built-in libraries:

- **Spark SQL**
- **Spark Streaming**
- **Spark MLlib** â†’ for Machine Learning
- **Spark GraphX** â†’ for graph-based processing
- Can also use **external libraries**

---

## ğŸ§© Components of Spark

### Low-Level APIs

```

RDD  â†’  Distributed Variables

```

- RDD (Resilient Distributed Dataset)
- Distributed variables (Broadcast, Accumulators)

---

### Structured APIs

```

DataFrame  â†’  Dataset  â†’  SQL

```

- Most commonly used APIs
- Table-like abstraction
- Schema-based

---

### Additional Capabilities

- Streaming
- Advanced analytics
- Rich libraries & ecosystem

---

## ğŸ—ï¸ Spark Architecture

### Spark Application
â†’ User-written code to process data

---

### Core Components

- **Driver Process**
- **Cluster Manager**
- **Executors**

---

### Driver Process

â†’ Acts as the **heart of the Spark application**  
â†’ Maintains all relevant application metadata  

Responsibilities:
- Runs the `main()` function
- Maintains application state
- Responds to user input
- Analyzes, distributes, and schedules work across executors
- Two-way communication with Cluster Manager

---

### Executors

â†’ Worker processes that:
- Execute tasks assigned by the Driver
- Perform actual computation
- Report results back to the Driver

---

### Cluster Manager

â†’ Manages **physical cluster resources**  
â†’ Allocates CPU, memory, and nodes  

Examples:
- YARN
- Kubernetes
- Standalone Spark

---

## ğŸŒ Spark Language APIs

â†’ Language in which Spark code is written  
â†’ Spark converts it into executable Spark jobs

Supported languages:
- **Scala** (native language of Spark)
- **Python** (supports all Scala constructs)
- **SQL**
- **R** (mainly for analytics)

---

## ğŸ§‘â€ğŸ’» Spark Session

â†’ Entry point to execute Spark code  
â†’ Gateway for all Spark functionalities  

### Types of Spark Session

```

Spark Session
/     
Interactive  Standalone App

````

---

### Interactive Mode

â†’ Spark session already available  
â†’ Used via terminal or shell

---

### Standalone Application

â†’ Spark session explicitly created  
â†’ Used in IDEs and production jobs

---

## ğŸ“Š Spark DataFrames & Partitioning

### DataFrames

â†’ Part of **Structured APIs**  
â†’ Table-like data structure  
â†’ Most widely used Spark abstraction  

Characteristics:
- Data organized into rows & columns
- Schema defines column names & types
- Data is partitioned & distributed across cluster nodes

---

### Partition

â†’ Collection of rows stored in **one physical chunk**  
â†’ Spark breaks data into smaller partitions  

Benefits:
- Enables parallelism
- Improves performance

---

## âš™ï¸ Parallelism in Spark

â†’ Parallelism depends on:
- Number of partitions
- Number of executors

Examples:
- **1 partition + 100 executors** â†’ Parallelism = 1  
- **Many partitions + 1 executor** â†’ Parallelism = 1  

âš ï¸ DataFrames do **not** allow manual manipulation of individual partitions

---

## ğŸ”„ Spark Transformations

### Immutable Data Structures

â†’ Spark data structures are **immutable**  
â†’ Any change produces a **new DataFrame / RDD**

---

### Transformation

â†’ Instruction used to modify data  
â†’ Does **not** execute immediately  
â†’ Builds execution lineage

Example:
```scala
df.where("number % 2 = 0")
````

â†’ This is a **filter transformation**

---

## ğŸ”€ Types of Transformations

```
Transformation
   /        \
Narrow      Wide
```

---

### Narrow Transformation

â†’ Each input partition contributes to **only one output partition**

Example:

* `map`
* `filter`

---

### Wide Transformation

â†’ Input partition contributes to **multiple output partitions**
â†’ Causes **shuffle**

Examples:

* `join`
* `groupBy`
* `reduceByKey`

---

## âœ… Summary

* Spark is a fast, in-memory processing engine
* Supports batch, streaming, and ML workloads
* Driver coordinates execution; executors perform work
* DataFrames are immutable and partitioned
* Transformations are lazy and form execution lineage

