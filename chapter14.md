# Apache Spark â€“ File Formats, Query Pushdown & Aggregations â€“ Clean Notes

---

## ğŸ“¦ Columnar File Formats

### Reading & Writing Parquet and ORC

â†’ **Parquet**
- Optimized for **Spark workloads**
- Best for long-term storage
- Columnar storage improves compression & query speed

â†’ **ORC**
- Optimized for **Hive workloads**
- Columnar format with efficient indexing

---

## ğŸ“„ Unstructured Files

â†’ Spark allows reading **unstructured files** such as text files  
â†’ Each line is treated as **one record**

Limitations:
- Only **one column** is allowed
- Cannot directly write multiple structured columns (e.g., integer columns) to text files

---

## ğŸ—„ï¸ Structured Files (Databases)

### JDBC & ODBC

â†’ **JDBC**
- Java Database Connectivity
- API used for database communication

â†’ **ODBC**
- Open Database Connectivity
- Introduced by Microsoft

---

### Reading from Databases (JDBC)

Key parameters:
- `format` â†’ `"jdbc"`
- `driver` â†’ e.g., `com.mysql.jdbc.Driver`
- `url`
- `tablename`
- `user`
- `password`

Notes:
- Schema does **not** need to be explicitly provided
- Spark automatically infers schema while reading

---

## ğŸ”½ Query Pushdown

â†’ Spark analyzes **logical and physical plans**  
â†’ Pushes filtering logic **down to the data source** whenever possible  

Benefits:
- Reduces data movement
- Filters data **before** creating DataFrames

Example:
- Instead of loading the entire table into Spark
- Spark pushes the query to the database and reads only the required subset

---

### Comparison

â†’ **RDBMS-level filtering**
- Subset of table created at database level

â†’ **Spark-level filtering**
- Entire table loaded first, then filtered in Spark

---

## ğŸ”€ Repartition vs Coalesce

### Repartition
â†’ Used to **increase or decrease** number of partitions  
â†’ **Shuffle occurs**  
â†’ Expensive operation

---

### Coalesce
â†’ Used only to **decrease** number of partitions  
â†’ No shuffle (or minimal shuffle)  
â†’ Combines existing partitions

âš ï¸ Avoid unnecessary shuffling as it is computationally expensive

---

## ğŸ“Š Spark Aggregations

To perform aggregation:
1. Specify **key column(s)**
2. Specify **aggregation function**

---

### Aggregation Types

```

Aggregation Types
|     |      |       |        |
Select GroupBy Window GroupingSet Rollup Cube

```

---

### Common Aggregate Functions

- `count()`
- `countDistinct()`
- `approx_count_distinct()` â†’ faster, approximate
- `first()`
- `last()`
- `max()`
- `min()`
- `sum()`
- `sumDistinct()`
- `avg()`

---

## ğŸ“ˆ Statistical Aggregate Functions

### Standard Deviation & Variance
- `var_pop()`
- `var_samp()`
- `stddev_pop()`
- `stddev_samp()`

---

### Skewness & Kurtosis
- `skewness()`
- `kurtosis()`

---

### Covariance & Correlation
- `corr()`
- `covar_samp()`
- `covar_pop()`

---

## ğŸ§® Grouping

â†’ Specify column(s) using `groupBy()`  
â†’ Aggregation applied using `agg()`

Notes:
- Each row belongs to **only one group**

---

## ğŸªŸ Window Functions

â†’ Used to perform aggregations over a **defined window**

### Difference from GroupBy

| Group By | Window Function |
|-------|----------------|
| Each row belongs to one group | Each row can belong to multiple windows |
| Reduces number of rows | Preserves original row count |

---

### Types of Window Functions

```

Spark Window Functions
/        |        
Aggregate  Analytical  Ranking

```

---

## ğŸ§± Rollup & Cube

### Rollup
â†’ Hierarchical / multidimensional aggregation  
â†’ Example:
- Total across all **dates â†’ countries**

---

### Cube
â†’ Computes **all possible aggregations**  
â†’ Produces all permutations & combinations  
â†’ More expensive than rollup

---

## âœ… Summary

- Parquet & ORC are efficient columnar formats
- Query pushdown minimizes data movement
- Repartition causes shuffle; coalesce avoids it
- Aggregations support statistical & analytical queries
- Window functions preserve row-level detail
- Rollup & Cube enable multidimensional analysis

