# ETL, Big Data & Hadoop Fundamentals â€“ Clean Notes

---

## ğŸ”§ ETL & Scheduling Tools

### ETL Tools
- Talend
- Informatica
- IBM DataStage

### Scheduling Tools
- Autosys
- Tidal
- Control-M

---

## ğŸ”„ Data Loading Types

â†’ **Historical / Initial Load**
- Used when a data warehouse is created for the first time
- Loads complete historical data

â†’ **Incremental Load**
- Loads only newly added data
- Usually runs:
  - Daily
  - Weekly
  - Fortnightly

---

## ğŸ“Š Big Data

### Sources of Data
- Machines
- People
- Organizations

### Emergence of Big Data
- All the **Vâ€™s**
- Cloud computing (computing anywhere)
- Advanced hardware

---

### Challenges in Big Data

- Machine-generated data â†’ complex
- Human-generated data â†’ unstructured
- Organization-generated data â†’ stored in silos

---

### Big Data Definition

â†’ Big data refers to a **huge amount of structured, semi-structured, and unstructured data**  
â†’ Cannot be stored or processed using traditional databases

---

### The 5 Vâ€™s of Big Data

â†’ **Volume**
- Size of the data generated and stored

â†’ **Velocity**
- Speed at which data is generated and accessed

â†’ **Variety**
- Structured (tables)
- Semi-structured (JSON, XML)
- Unstructured (images, videos)

â†’ **Veracity**
- Accuracy and reliability of data

â†’ **Value**
- How useful the data is for decision-making

---

## ğŸ˜ Hadoop Ecosystem

â†’ A **platform / suite** that provides various services to handle big data  
â†’ Consists of multiple tools working together

---

## ğŸ“‚ Distributed File System

â†’ Distributes files **across multiple machines** over a network  
â†’ Enables fault tolerance and scalability

---

## ğŸ§© Commodity Cluster

â†’ An ensemble of **fully independent computing systems**
â†’ Integrated using **commodity hardware** over a network

### Characteristics
- **Affordable** â†’ Easy to procure
- **Scalable** â†’ Capacity can be increased
- **Flexible** â†’ Can scale up or down

---

## ğŸ¯ Central Theme of Big Data

â†’ **Distributed Storage**
- Data stored across multiple systems

â†’ **Distributed Processing**
- Code is sent to where data resides
- Avoids moving large data to computation nodes
- Based on **data locality**

Reasons:
1. Computation is parallelized
2. Multiple processors handle data efficiently

---

## ğŸ§  Distributed Processing Model

â†’ Distributed processing is achieved through a **programming model**

â†’ Programming model consists of:
- Runtime libraries
- Interfaces
- High-level and low-level languages

---

## ğŸ§ª Programming Model Requirements for Big Data

â†’ Programmability over distributed file systems

### Requirements
1. Support big data operations
2. Handle fault tolerance
3. Support adding more nodes
4. Optimized for specific data types

---

## âš™ï¸ Big Data Operations

1. Split volumes of data
2. Access data fast
3. Distribute computations to nodes

---

## ğŸ›¡ï¸ Fault Tolerance

â†’ Achieved by:
1. Replicating data partitions
2. Recovering files when needed

---

## â• Adding More Nodes (Scalability)

### Scaling Out
- Adding more machines (nodes)
- System continues to work efficiently

### Vertical Scaling (Scaling Up)
- Increasing capacity of a single system
- Adding more CPU, memory, or storage

### Horizontal Scaling (Scaling Out)
- Connecting more systems together
- Cost-effective
- Preferred for big data systems

---

## âš¡ Optimized for Specific Data

â†’ Optimization means achieving results:
- With minimal resources
- In minimal time

â†’ Programming model is designed for **huge data volumes**

âš ï¸ Not all programming models work for all types of data

---
