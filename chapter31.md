# Performance & Cost Optimization (Spark, Kafka, AWS) â€“ Chapter 31

---

## ğŸ¯ Why Optimization Matters in Data Engineering

â†’ Data Engineering is not just about correctness  
â†’ It is about **speed, scalability, and cost control**

Interviewers test:
- Performance bottlenecks
- Trade-offs
- Cost-awareness

---

# âš¡ Apache Spark Optimization

---

## ğŸ§  Spark Performance Pillars

```

CPU
Memory
Disk
Network (Shuffle)

````

â†’ Most Spark jobs fail or slow down due to **shuffle & memory issues**

---

## ğŸ” Shuffle Optimization (VERY IMPORTANT)

â†’ Shuffle happens during:
- Joins
- GroupBy
- ReduceByKey
- Window functions

### Reduce Shuffle
- Prefer **broadcast joins**
- Filter data BEFORE joins
- Reduce number of columns early

---

### Shuffle Partitions

Default:
```text
spark.sql.shuffle.partitions = 200
````

â†’ Too many partitions:

* Small tasks
* Scheduler overhead

â†’ Too few partitions:

* Large tasks
* OOM errors

ğŸ‘‰ Tune based on data size

---

## ğŸ“¦ Partitioning Strategies

### Repartition

â†’ Increases or decreases partitions
â†’ Causes full shuffle

```python
df.repartition(100)
```

---

### Coalesce

â†’ Reduces partitions
â†’ No shuffle (faster)

```python
df.coalesce(10)
```

---

## ğŸ§  Broadcast Join Optimization

â†’ Small table is sent to all executors
â†’ Avoids shuffle

```sql
SELECT /*+ BROADCAST(dim) */ *
FROM fact
JOIN dim ON fact.id = dim.id;
```

âš ï¸ Use only when table fits in memory

---

## ğŸ’¾ Memory Management

â†’ Spark memory types:

* Execution memory
* Storage memory

Best practices:

* Cache only reused data
* Unpersist after use
* Avoid caching large unnecessary datasets

---

## ğŸ§  Persist vs Cache

* `cache()` â†’ Memory only
* `persist()` â†’ Memory + disk options

---

## ğŸ” Spark UI (INTERVIEW GOLD)

â†’ Use Spark UI to analyze:

* Stage execution
* Shuffle read/write
* Skewed partitions
* Task failures

---

## âš ï¸ Data Skew

â†’ One partition holds most data

Symptoms:

* Few tasks run very long
* Others finish quickly

Solutions:

* Salting keys
* Broadcast joins
* Repartition on better key

---

# ğŸŒŠ Kafka Optimization

---

## ğŸ§± Partition Strategy

â†’ Partitions control:

* Parallelism
* Throughput

Rule:

```
More partitions â†’ More parallelism
```

âš ï¸ Too many partitions:

* Broker overhead
* Memory pressure

---

## ğŸ”‘ Partition Key Choice

â†’ Bad key â†’ data skew
â†’ Good key â†’ even distribution

Examples:

* user_id âŒ (hot users)
* hashed(user_id) âœ…

---

## â±ï¸ Consumer Performance

Tune:

* `max.poll.records`
* `fetch.min.bytes`
* `fetch.max.wait.ms`

Goal:

* Balance latency vs throughput

---

## ğŸ” Offset Commit Strategy

* Auto commit â†’ simple, risky
* Manual commit â†’ safer, controlled

ğŸ‘‰ Prefer **manual commits** in critical pipelines

---

## â˜ï¸ AWS Cost Optimization

---

## ğŸª£ Amazon S3 Optimization

### Storage Classes

| Class        | Use Case          |
| ------------ | ----------------- |
| Standard     | Frequent access   |
| IA           | Infrequent access |
| Glacier      | Archive           |
| Deep Archive | Long-term         |

â†’ Use **Lifecycle policies** to move data automatically

---

## ğŸ’» EC2 Optimization

### Instance Types

* Compute optimized
* Memory optimized
* Storage optimized

ğŸ‘‰ Choose based on workload

---

### Spot vs On-Demand

* **On-Demand**
  â†’ Stable, expensive

* **Spot**
  â†’ Cheap, interruptible

Use cases:

* Batch jobs
* Non-critical processing

---

## âš™ï¸ EMR vs Glue vs Lambda

| Service | Best Use                |
| ------- | ----------------------- |
| EMR     | Long-running big jobs   |
| Glue    | Serverless ETL          |
| Lambda  | Small event-driven jobs |

---

## ğŸ“Š Data Warehouse Optimization

---

### Partitioning

â†’ Partition by:

* Date
* Region
* Customer segment

Benefits:

* Partition pruning
* Faster queries
* Lower cost

---

### Clustering

â†’ Sort data on frequently filtered columns

Benefits:

* Faster scans
* Reduced IO

---

## ğŸ’° Cost-Aware Querying

â†’ Avoid:

* SELECT *
* Unfiltered joins
* Cross joins

â†’ Always:

* Filter early
* Select only required columns

---

## ğŸ§  Interview Optimization Questions

* How do you optimize Spark joins?
* What causes shuffle?
* How do you handle data skew?
* How do you reduce AWS cost?
* How do partitions affect Kafka throughput?
* When would you use Spot instances?

---

## âœ… Chapter 31 Summary

* Spark bottlenecks = shuffle + memory
* Partitioning strategy is critical
* Broadcast joins improve performance
* Kafka partitions drive parallelism
* Consumer tuning improves throughput
* AWS costs must be actively optimized
* Storage & compute choices matter

