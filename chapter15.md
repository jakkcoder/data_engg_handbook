# Apache Spark ‚Äì Joins, Join Strategies & RDD Internals ‚Äì Clean Notes

---

## üîó Spark Joins

‚Üí Same as SQL joins  
‚Üí Difference: **DataFrames are joined instead of tables**

---

## üîÄ Types of Joins in Spark

```

Join Types
|      |        |          |         |          |        |
Inner  Outer   Left Outer  Right Outer Left Semi Left Anti Cross

```

---

### Inner Join
‚Üí Returns **only matching records**
‚Üí Common values in both DataFrames

---

### Outer Join
‚Üí Returns **all records** from both DataFrames  
‚Üí Non-matching values are filled with `null`

---

### Left Outer Join
‚Üí All records from **left DataFrame**  
‚Üí Matching records from right DataFrame

---

### Right Outer Join
‚Üí All records from **right DataFrame**  
‚Üí Matching records from left DataFrame

---

### Left Semi Join
‚Üí Used mainly as a **filter**
‚Üí Returns only columns from **left DataFrame**
‚Üí Keeps rows that have a match in right DataFrame
‚Üí Right DataFrame columns are **not included**

---

### Left Anti Join
‚Üí Returns rows from **left DataFrame that do NOT match**
‚Üí Used for identifying missing records

---

### Cross Join
‚Üí Produces **Cartesian product**
‚Üí Every row from left joined with every row from right
‚ö†Ô∏è Very expensive ‚Äî use carefully

---

## ‚ö†Ô∏è Challenges in Spark Joins

### Joins on Complex Data Types
‚Üí Complex columns (array / struct) cannot be directly joined  
‚Üí Solution:
- Use `explode()` before join

---

### Duplicate Column Names
Ways to handle:
1. Use **different join expressions**
2. **Drop** duplicate columns after join
3. **Rename** columns before joining

---

## ‚öôÔ∏è Spark Join Strategies

‚Üí Communication between nodes happens in **two ways**

```

Join Strategies
/                
Shuffle Join     Broadcast Join

````

---

### Shuffle Join
‚Üí Used for **big table ‚Üî big table**
‚Üí Data shuffled across network
‚Üí Expensive due to data transfer
‚Üí Default strategy if no optimization applies

---

### Broadcast Join
‚Üí Used for **big table ‚Üî small table**
‚Üí Small table is broadcast to all executors
‚Üí Big table does not shuffle
‚Üí Much faster than shuffle join

---

## üß† Spark Join Execution (Logical vs Physical)

‚Üí Spark decides join strategy based on:
- Data size
- Configuration
- Statistics

---

## üß© Different Join Algorithms

- **Broadcast Hash Join**
- **Shuffle Hash Join**
- **Shuffle Sort-Merge Join**
- **Broadcast Nested Loop Join**
- **Cartesian Join**

---

## üß™ Low-Level APIs in Spark

‚Üí Used when Structured APIs (DataFrame/Dataset) are insufficient

### Why Low-Level APIs?

- Need fine control over **data placement**
- Custom transformations
- Maintain legacy codebases
- Advanced debugging

---

## üß± RDD (Resilient Distributed Dataset)

‚Üí Immutable, **partitioned collection of records**
‚Üí Operated on in **parallel**
‚Üí Low-level Spark abstraction

---

### RDD Characteristics

- Immutable
- Distributed
- Fault tolerant
- Supports parallel operations

---

### Records in RDD

‚Üí Records are **language objects**
- Java objects
- Scala objects

---

### Limitations of RDD

- No automatic optimization
- No Catalyst optimizer
- No Tungsten execution engine
- More verbose code

---

## üîÅ Spark Context

‚Üí `sc` is the short form of **SparkContext**

Examples:
```scala
sc.parallelize(0 to 9)
sc.parallelize(0 to 9, 10)
````

---

## üß¨ Properties of RDD

### Mandatory Properties

* List of partitions
* Function to compute each split
* List of dependencies on other RDDs

### Optional Properties

* Partitioner (for key-value RDDs)
* Preferred locations for computation

---

## üß© Types of RDD

```
RDD
 /         \
Generic   Key-Value
```

‚Üí Key-Value RDD supports more **functional operations**

---

## ‚ùì When to Use RDD

‚Üí Avoid using RDD unless required
‚Üí Use when:

* Explicit partition control is needed
* Low-level manipulation required

---

## üì• Creating RDDs

1. Convert DataFrame to RDD (**most preferred**)
2. From local collection
3. From external data source (not recommended)

---

## üîó RDD Lineage

‚Üí Graph of all **parent RDDs**
‚Üí Enables **fault tolerance**

Why lineage is useful:

1. Supports in-memory processing
2. Enables recomputation instead of replication
3. Filters can be pushed earlier (optimization)

---

### Failure Recovery

‚Üí If an RDD partition is lost:

* Spark **recreates it from lineage**

---

## üîÑ RDD Transformations

‚Üí Manipulation done using **language-level functions**
‚Üí User must explicitly write logic

### Common Transformations

```
Transformations
 |       |      |        |      |      |
Distinct Filter  Map   FlatMap  Sort  Split
```

Examples:

* `distinct()`
* `filter(word => word.startsWith("a"))`
* `map(word => (word, 1))`
* `flatMap()`

---

## ‚ñ∂Ô∏è RDD Actions

```
Actions
 |        |      |        |        |
Reduce  Count  First  Max/Min   Take
```

Notes:

* `reduce()` is **not deterministic**
* `take()`, `takeOrdered()`, `top()` are commonly used

---

## ‚úÖ Summary

* Spark joins behave like SQL joins on DataFrames
* Broadcast joins are fastest for small tables
* Shuffle joins are expensive
* RDDs provide low-level control but lack optimization
* Prefer DataFrames unless RDDs are strictly required
