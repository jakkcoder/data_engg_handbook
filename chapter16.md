# Apache Spark ‚Äì Joins, Join Strategies & RDD Internals (Clean Notes)

---

## üîó Spark Join

‚Üí Same as SQL joins  
‚Üí Difference: **instead of tables, DataFrames are joined**

---

## üîÄ Join Types

```

Join Types
|      |        |           |            |           |        |
Inner  Outer   Left Outer   Right Outer   Left Semi   Left Anti  Cross

```

### Inner Join
‚Üí Returns **only matching records**

### Outer Join
‚Üí Returns **all records from both sides**
‚Üí Non-matching values are filled with `null`

### Left Outer Join
‚Üí All records from **left DataFrame**
‚Üí Matching records from right DataFrame

### Right Outer Join
‚Üí All records from **right DataFrame**
‚Üí Matching records from left DataFrame

### Left Semi Join
‚Üí Used **more like a filter**
‚Üí Only matching rows from **left table**
‚Üí Only **left table columns** appear in result

### Left Anti Join
‚Üí Returns **non-matching records**
‚Üí Only records from **left table** that do NOT match

### Cross Join
‚Üí Cartesian product
‚Üí Every row from left joins with every row from right
‚ö†Ô∏è Very expensive

---

## ‚ö†Ô∏è Challenges in Joins

### Joins on Complex Types
‚Üí Complex columns (array / struct) cannot be joined directly  
‚Üí Solution:
```

explode() ‚Üí then join

```

---

### Handling Duplicate Columns

Options:
1. Use **different join expressions**
2. **Drop** duplicate columns after join
3. **Rename** columns before join

---

## ‚öôÔ∏è Spark Join Strategies

‚Üí Communication in Spark happens in **two ways**

```

Join Strategies
/            
Shuffle Join   Broadcast Join

````

---

### Shuffle Join
‚Üí **Big table ‚Üî Big table**
‚Üí Data shuffled across the network
‚Üí Expensive because **data transfer occurs**

---

### Broadcast Join
‚Üí **Big table ‚Üî Small table**
‚Üí Small table is **broadcast to all executors**
‚Üí Big table does not shuffle
‚Üí Much faster than shuffle join

---

## üß† Different Join Algorithms

‚Üí Spark can choose among:

- Broadcast Hash Join
- Shuffle Hash Join
- Shuffle Sort-Merge Join
- Broadcast Nested Loop Join
- Cartesian Join

‚Üí Selection depends on:
- Data size
- Statistics
- Spark configuration

---

## üß™ Low-Level APIs

### RDD ‚Äì Manipulating Distributed Data

‚Üí Used when functionality **cannot be done using Structured APIs**  
‚Üí Provides control over **physical data placement** across cluster  

Why low-level APIs?
- Maintain **legacy codebases**
- Custom data manipulation
- Easier debugging in some scenarios

‚ö†Ô∏è Spark internally converts DataFrames to RDDs,  
but **users should not rely on RDDs unless required**

---

## üß± RDD (Resilient Distributed Dataset)

‚Üí Immutable, partitioned collection of records  
‚Üí Can be processed in parallel  

Characteristics:
- Immutable
- Distributed
- Fault tolerant

Notes:
- In RDD, records are **language objects** (Java / Scala)
- No automatic optimization (unlike DataFrames)
- No Catalyst optimizer

---

## üîë Spark Context

‚Üí `sc` = **SparkContext**

Examples:
```scala
sc.parallelize(0 to 9)
sc.parallelize(0 to 9, 10)
````

---

## üß¨ Properties of RDD

### Mandatory

* List of partitions
* Function to compute each split
* List of dependencies on other RDDs

### Optional

* Partitioner (for key-value RDD)
* Preferred locations for computation

---

## üß© Types of RDD

```
RDD
 /        \
Generic   Key-Value
```

‚Üí Key-Value RDD has **more functional operations**

---

## ‚ùì When to Use RDD

‚Üí **Do not use RDD unless required**

Use RDD when:

* Explicit partition control is needed
* Low-level transformations are required

---

## üì• Creating RDDs

Preferred order:

1. **Convert DataFrame to RDD** (most preferable)
2. RDD from local collection
3. RDD from data source (not advisable)

---

## üîó RDD Lineage

‚Üí Graph of all **parent RDDs** of an RDD
‚Üí Enables **fault tolerance**

Why lineage?

1. Supports in-memory processing
2. If a partition is lost, Spark **recreates it**
3. Enables optimization (e.g., filter pushdown)

---

## üîÑ RDD Transformations

‚Üí Manipulation done using **language-level functions**
‚Üí User explicitly writes logic

### Types of Transformations

```
Transformations
 |        |      |         |      |      |
Distinct Filter  Map    FlatMap  Sort   Split
```

Examples:

* `distinct()`
* `filter(word => word.startsWith("a"))`
* `map(word => (word, 1))`
* `flatMap()`

---

## ‚ñ∂Ô∏è RDD Actions

```
Actions
 |        |      |        |        |
Reduce  Count  First  Max/Min   Take
```

Notes:

* `reduce()` is **not deterministic**
* Common actions:

  * `take()`
  * `takeOrdered()`
  * `top()`

---

## ‚úÖ Summary

* Spark joins mirror SQL joins on DataFrames
* Broadcast join is fastest for small tables
* Shuffle join is expensive due to data transfer
* RDDs provide low-level control but lack optimization
* Prefer DataFrames unless RDDs are absolutely required

---
