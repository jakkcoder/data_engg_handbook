# Data Quality, Reliability & Monitoring (Chapter 30)

---

## ğŸ¯ Why Data Quality Matters in Data Engineering

â†’ Data pipelines are successful **only if data is trusted**

Bad data causes:
- Wrong business decisions
- Loss of credibility
- Reprocessing & downtime

âš ï¸ Interviewers expect Data Engineers to **own data quality**

---

## ğŸ§  Data Quality vs Data Reliability

| Data Quality | Data Reliability |
|---|---|
Correctness of data | Consistency & availability |
Accuracy | Timely delivery |
Completeness | Fault tolerance |

---

## ğŸ§ª Core Data Quality Dimensions

---

### âœ… Completeness
â†’ Are all required records present?

Example:
- Missing orders
- Missing customer IDs

---

### âœ… Accuracy
â†’ Does data reflect real-world values?

Example:
- Incorrect prices
- Wrong timestamps

---

### âœ… Consistency
â†’ Same data should match across systems

Example:
- Revenue mismatch between tables

---

### âœ… Timeliness
â†’ Is data arriving **on time**?

Example:
- Daily report delayed

---

### âœ… Uniqueness
â†’ No duplicate records

Example:
- Same order_id appearing multiple times

---

## ğŸ” Common Data Quality Checks (INTERVIEW MUST)

---

### Null Checks
â†’ Critical columns should not be NULL

```sql
WHERE customer_id IS NULL
````

---

### Duplicate Checks

â†’ Identify duplicate primary keys

```sql
COUNT(*) > 1
```

---

### Range Checks

â†’ Values within expected range

Example:

* Price > 0
* Age between 0â€“120

---

### Referential Integrity

â†’ Foreign keys must exist in dimension tables

Example:

* fact_orders.customer_id exists in dim_customer

---

### Schema Validation

â†’ Ensure schema has not changed unexpectedly

Examples:

* Missing columns
* Wrong data types

---

## ğŸ“ Data Freshness

â†’ Measures **how recent data is**

Example:

* Last updated timestamp

```sql
MAX(updated_at)
```

âš ï¸ Stale data is often worse than no data

---

## â±ï¸ SLAs, SLOs & SLIs (INTERVIEW FAVORITE)

---

### SLA (Service Level Agreement)

â†’ Business-level promise

Example:

* Data available by 9 AM

---

### SLO (Service Level Objective)

â†’ Target metric

Example:

* 99% daily jobs finish on time

---

### SLI (Service Level Indicator)

â†’ Actual measurement

Example:

* Job completion time

---

## ğŸ”„ Handling Pipeline Failures

â†’ Failures WILL happen

Reasons:

* Network issues
* Bad source data
* Resource exhaustion

---

### Failure Handling Strategies

* Retries with backoff
* Partial reruns
* Partition-level reruns
* Alerts & notifications

---

## ğŸ§  Exactly-Once vs At-Least-Once

---

### At-Least-Once

â†’ May process duplicates
â†’ Easier to implement

Requires:

* Deduplication logic

---

### Exactly-Once

â†’ Each record processed once
â†’ Harder to achieve

Requires:

* Checkpointing
* Idempotent writes
* Transactional sinks

---

## ğŸ” Idempotency (REINFORCE)

â†’ Running same pipeline multiple times gives same result

Techniques:

* MERGE / UPSERT
* Partition overwrite
* Unique constraints
* Deduplication windows

---

## ğŸ“Š Monitoring & Observability

---

### What to Monitor?

* Job duration
* Row counts
* Failure rates
* Data freshness
* Data volume anomalies

---

### Monitoring Tools

* Airflow UI
* CloudWatch
* Stackdriver
* Custom dashboards

---

## ğŸš¨ Alerting

â†’ Alerts should be:

* Actionable
* Not noisy

Trigger alerts on:

* Job failures
* SLA misses
* Data anomalies
* Missing partitions

---

## ğŸ§  Data Validation Layers

```
Source â†’ Ingestion â†’ Transformation â†’ Serving
   |          |              |           |
   QC         QC             QC          QC
```

â†’ Validate at **every stage**

---

## ğŸ§ª Data Reconciliation

â†’ Ensure data matches across systems

Examples:

* Source vs warehouse row count
* Daily totals comparison

---

## ğŸ” Security & Data Quality

â†’ Bad permissions can cause:

* Missing data
* Partial loads

Ensure:

* Proper IAM roles
* Least privilege access

---

## ğŸ§  Interview Design Questions

* How do you ensure data quality?
* What checks do you apply in pipelines?
* How do you handle SLA breaches?
* How do you detect bad data early?
* How do you recover from failures?

---

## âœ… Chapter 30 Summary

* Data Engineers own data quality
* Quality dimensions must be enforced
* SLAs & freshness are critical
* Failures require controlled recovery
* Idempotency prevents duplicates
* Monitoring & alerting are mandatory
* Validation must exist at every stage
