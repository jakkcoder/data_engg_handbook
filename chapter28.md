# ETL vs ELT & Data Pipeline Design Patterns (Chapter 28)

---

## ğŸ¯ Why ETL / ELT Matters in Data Engineering

â†’ Data Engineers are judged on **how they move, transform, and trust data**

Interviewers look for:
- Correct pipeline choice
- Scalability
- Reliability
- Cost awareness

---

## ğŸ”„ ETL vs ELT (FREQUENTLY ASKED)

```

ETL â†’ Transform BEFORE loading
ELT â†’ Transform AFTER loading

```

---

## ğŸ§± ETL (Extract â€“ Transform â€“ Load)

### Flow
```

Source â†’ Transform â†’ Target

```

### Characteristics
- Transformations happen **outside the data warehouse**
- Requires compute before load
- Slower for very large datasets

### When ETL is Used
- Legacy systems
- Limited warehouse compute
- Strict transformation logic

Examples:
- Informatica
- Talend
- Custom Spark jobs

---

## ğŸ§± ELT (Extract â€“ Load â€“ Transform)

### Flow
```

Source â†’ Load â†’ Transform

```

### Characteristics
- Raw data loaded first
- Transformations run **inside warehouse**
- Scales better with cloud warehouses

### When ELT is Used (MODERN STANDARD)
- Cloud-native systems
- BigQuery / Snowflake / Redshift
- Flexible analytics use cases

Examples:
- dbt
- Spark SQL
- Warehouse-native SQL

ğŸ‘‰ **ELT is preferred in modern DE interviews**

---

## ğŸ§  ETL vs ELT Comparison

| ETL | ELT |
|---|---|
Transform before load | Transform after load |
External compute | Warehouse compute |
Rigid | Flexible |
Harder to scale | Cloud-friendly |

---

## ğŸ•’ Batch vs Streaming Pipelines

---

## ğŸ“¦ Batch Processing

â†’ Data processed at **intervals**

Examples:
- Daily sales reports
- Nightly ETL jobs

Characteristics:
- High latency
- Easier to debug
- Cost-effective

---

## ğŸŒŠ Streaming Processing

â†’ Data processed **as it arrives**

Examples:
- Fraud detection
- Real-time dashboards

Characteristics:
- Low latency
- Complex state management
- Harder to debug

---

## ğŸ§  Batch vs Streaming

| Batch | Streaming |
|---|---|
High latency | Low latency |
Simple logic | Complex logic |
Cheap | Expensive |
Easy backfills | Hard backfills |

---

## ğŸ§ª Idempotent Pipelines (VERY IMPORTANT)

â†’ Running pipeline **multiple times gives same result**

Why needed:
- Retries
- Failures
- Backfills

### Idempotency Techniques
- Use primary keys
- Use merge / upsert
- Delete + insert by partition
- Deduplication logic

---

## ğŸ” Backfills & Reprocessing

â†’ Used when:
- Bug fixes
- Late data
- Logic changes

### Safe Backfill Strategy
1. Isolate affected date range
2. Re-run pipeline only for that range
3. Validate output
4. Promote to production

âš ï¸ Never blindly backfill entire dataset

---

## â³ Late-Arriving Data

â†’ Data arrives **after expected processing window**

Examples:
- Delayed events
- System outages

### Handling Strategies
- Watermarks (streaming)
- Reprocessing recent partitions
- SCD Type 2 updates

---

## ğŸ§© Schema Evolution

â†’ Schema changes over time

Examples:
- New column added
- Column removed
- Data type change

### Safe Practices
- Backward-compatible changes
- Nullable new columns
- Versioned schemas
- Avoid breaking changes

---

## ğŸ§  Incremental vs Full Load

---

### Full Load
â†’ Load entire dataset every run

Pros:
- Simple logic

Cons:
- Expensive
- Slow

---

### Incremental Load (PREFERRED)

â†’ Load only **new or changed data**

Methods:
- Timestamp-based
- ID-based
- CDC-based

---

## ğŸ” Change Data Capture (CDC)

â†’ Capture only changed records from source

Types:
- Inserts
- Updates
- Deletes

Tools:
- Debezium
- Database logs
- Kafka-based CDC

---

## ğŸ§ª Data Validation in Pipelines

â†’ Validate data at every stage

Checks:
- Row counts
- Null checks
- Duplicate checks
- Range checks

---

## ğŸ§  Exactly-Once vs At-Least-Once

### At-Least-Once
â†’ Possible duplicates  
â†’ Requires deduplication

### Exactly-Once
â†’ Harder to achieve  
â†’ Requires:
- Idempotency
- Checkpointing

---

## âš¡ Pipeline Failure Handling

â†’ Pipelines WILL fail

Best practices:
- Retries with backoff
- Partial reruns
- Alerts & monitoring
- Idempotent logic

---

## ğŸ§  Interview Design Question Examples

- How would you design a daily ETL pipeline?
- How do you handle late-arriving data?
- How do you backfill safely?
- When do you choose streaming over batch?
- How do you make pipelines idempotent?

---

## âœ… Chapter 28 Summary

- ELT is modern standard
- Batch is simpler; streaming is faster
- Idempotency is critical
- Backfills must be controlled
- Late data must be handled explicitly
- Incremental loads are preferred
- CDC improves efficiency
